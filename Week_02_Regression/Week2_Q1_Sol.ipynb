{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2s3I0GAfXHA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGrsLyROfjdh"
   },
   "outputs": [],
   "source": [
    "class nlr:\n",
    "  # Evaluates the gradient of cost function (J). Hint: You can use this to optimize w\n",
    "  def grad(self,x,y,w):\n",
    "    m = y.size    # number of training examples\n",
    "    grad_J = (1/m)*np.dot((np.dot(x,w) - y),x) \n",
    "    return grad_J\n",
    "\n",
    "  # This function calculates the cost (J)\n",
    "  def computeCost(self,x,y,w):\n",
    "    J = 0    # J is cost function\n",
    "    m = y.size\n",
    "    J =(1/(2*m))*np.sum(np.square(np.matmul(x,w) - y)) # write your code to calculate J\n",
    "    return J\n",
    "  \n",
    "  # This function optimizes the weights w_0, w_1, w_2. Batch Gradient Descent method\n",
    "  def BgradientDescent(self, x, y, w, alpha, iters):\n",
    "    m = y.size   \n",
    "    w = w.copy() # To keep a copy of original weights\n",
    "    \n",
    "    J_history = []   # Use a python list to save cost in every iteration\n",
    "\n",
    "    for i in range(iters):\n",
    "      # Loop to update weights (w vector)\n",
    "      # Also save cost at every step\n",
    "      w = w - (alpha)*self.grad(x,y,w)\n",
    "        \n",
    "        # save the cost J in every iteration\n",
    "      J_history.append(self.computeCost(x, y, w))\n",
    "    return w, J_history\n",
    "  \n",
    "  # This function optimizes the weights w_0, w_1, w_2. Stochastic Gradient Descent method\n",
    "  def SgradientDescent(self, x, y, w, alpha, iters):\n",
    "    m = y.size  # number of training examples\n",
    "    w = w.copy() # To keep a copy of original weights\n",
    "    \n",
    "    J_history_s = []   # Use a python list to save cost in every iteration\n",
    "\n",
    "    for i in range(iters):\n",
    "      #r = # generate random integers (refer lab demo code)\n",
    "      # create randomly a minibatch from whole data set and find weights based on that new data set.\n",
    "      # Loop to update weights (w vector)\n",
    "      # Also save cost at every step\n",
    "      ri = np.random.randint(m)\n",
    "      err = (np.dot((np.dot(x[ri:ri+10],w) - y[ri:ri+10]),x[ri:ri+10]))\n",
    "      w = w - (alpha/m)*err\n",
    "        \n",
    "            # save the cost J in every iteration\n",
    "      J_history_s.append(self.computeCost(x, y, w))\n",
    "\n",
    "    return w, J_history_s\n",
    "  \n",
    "  # This function implements line search Secant method\n",
    "  # refer to class notes on optimization and lab demo copy.\n",
    "  def ls_secant(self,x,y,w,d):\n",
    "    # d is search direction d = -grad(J). Refer class and Lab notes\n",
    "    epsilon = 10**(-4) # Line search tolerance\n",
    "    \n",
    "    alpha_curr = 0     # Alpha (x_i-1)\n",
    "    alpha = 0.01       # initial value (x_i)\n",
    "\n",
    "    dphi_zero = np.dot(d,self.grad(x,y,w))         # dphi_zero = (d^T)(grad J(w_0) # At every alpha updation loop you will have a given initial weight vector (w_0)\n",
    "    dphi_curr = dphi_zero  # required for first alpha iteration\n",
    "    i = 0\n",
    "    while abs(dphi_curr) > (epsilon*abs(dphi_zero)):  # tolerance or looping criteria used here\n",
    "      # write loop to update alpha\n",
    "      alpha_old = alpha_curr                     # alpha(x_k)\n",
    "      alpha_curr = alpha                      # alpha(x_k+1)\n",
    "      dphi_old = dphi_curr\n",
    "      w1 = w -alpha_curr*d                       # Evaluate (updated_w). Used w1 to avoid update of w (main).\n",
    "      dphi_curr = np.dot(d,self.grad(x ,y,w1))        # Evaluate grad J(updated_w)\n",
    "      alpha = (dphi_curr*alpha_old - dphi_old*alpha_curr)/(dphi_curr - dphi_old)  #secant method expanded form\n",
    "        #print('Inside loop alpha',alpha)\n",
    "      i = i+1\n",
    "\n",
    "    return alpha\n",
    "\n",
    "  # This function optimizes the weights w_0, w_1, w_2. Batch Gradient Descent method using variable alpha which you previously updated using ls_secant() method\n",
    "  def AgradientDescent(self,x, y, w, iters):\n",
    "    m = y.size  # number of training examples\n",
    "    w = w.copy() # To keep a copy of original weights\n",
    "    eps = 10**(-12); # tolerance for J_history\n",
    "\n",
    "    J_his = [0]   # Use a python list to save cost in every iteration\n",
    "\n",
    "    for i in range(iters):\n",
    "      d = (-1)*self.grad(x,y,w) # d is search direction d = -grad(J)\n",
    "      alpha = self.ls_secant(x,y,w,d) # update alpha at every iteration\n",
    "  \n",
    "      # Loop to update weights (w vector)\n",
    "      # Also save cost at every step in J_history_a\n",
    "      w = w - (alpha)*d\n",
    "      J_his.append(self.computeCost(x, y, w))\n",
    "      \n",
    "      # stopping criteria\n",
    "      if (J_his[i+1] -J_his[i]) < eps:\n",
    "        print('No. of iterations',i)\n",
    "        break\n",
    "\n",
    "    return w, J_his"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Week2_Q1_Test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
