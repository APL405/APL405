{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 04: Multi-class Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "In this exercise, we will implement logistic regression based multiclass classification to recognize handwritten digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# Module to load MATLAB mat datafile format (Input and output module of scipy)\n",
    "\n",
    "\n",
    "# Python Imaging Library (PIL)\n",
    "\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Multi-class Classification\n",
    "\n",
    "For this exercise, logistic regression will be used to recognize handwritten digits (from 0 to 9).\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The data set is given in `Week4_data.mat` that contains 5000 training examples of handwritten digits. Use the function `loadmat` within the `scipy.io` module to load the data.\n",
    "\n",
    "There are 5000 training examples in `Week4_data.mat`, where each training example is a 20 pixel by 20 pixel grayscale image of the digit. Each pixel is represented by a floating point number indicating the grayscale intensity at that location. The 20 by 20 grid of pixels is “unrolled” into a 400-dimensional vector. Each of these training examples becomes a single row in our data matrix `X`. This gives us a 5000 by 400 matrix `X` where every row is a training example for a handwritten digit image.\n",
    "\n",
    "$$ X = \\begin{bmatrix} - \\: (x^{(1)})^T \\: - \\\\ -\\: (x^{(2)})^T \\:- \\\\ \\vdots \\\\ - \\: (x^{(m)})^T \\:-  \\end{bmatrix} $$\n",
    "\n",
    "The second part of the training set is a 5000-dimensional vector `y` that contains labels for the training set.\n",
    "\n",
    "![](Figures/MultiClass.png)\n",
    "\n",
    "![](Figures/reshape.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 labels, from 1 to 10 (note that we have mapped \"0\" to label 10)\n",
    "num_labels = 10\n",
    "\n",
    "data = \n",
    "X, y = \n",
    "\n",
    "# set the zero digit to 0, rather than its mapped 10 in this dataset\n",
    "\n",
    "\n",
    "m = y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MATLAB Data\n",
    "\n",
    "![](data11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of useful functions that are going to be used thoughout the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayData(X, example_width=None, figsize=(10, 10)):\n",
    "    \"\"\"\n",
    "    Displays 2D data stored in X in a nice grid.\n",
    "    \"\"\"\n",
    "    # Compute rows, cols\n",
    "    if X.ndim == 2:\n",
    "        m, n = X.shape\n",
    "    elif X.ndim == 1:\n",
    "        n = X.size\n",
    "        m = 1\n",
    "        X = X[None]  # Promote to a 2 dimensional array\n",
    "    else:\n",
    "        raise IndexError('Input X should be 1 or 2 dimensional.')\n",
    "\n",
    "    example_width = example_width or int(np.round(np.sqrt(n)))\n",
    "    example_height = n / example_width\n",
    "\n",
    "    # Compute number of items to display\n",
    "    display_rows = int(np.floor(np.sqrt(m)))\n",
    "    display_cols = int(np.ceil(m / display_rows))\n",
    "\n",
    "    fig, ax_array = pyplot.subplots(display_rows, display_cols, figsize=figsize)\n",
    "    fig.subplots_adjust(wspace=0.025, hspace=0.025)\n",
    "\n",
    "    ax_array = [ax_array] if m == 1 else ax_array.ravel()\n",
    "\n",
    "    for i, ax in enumerate(ax_array):\n",
    "        ax.imshow(X[i].reshape(example_width, example_width, order='F'),\n",
    "                  cmap='Greys', extent=[0, 1, 0, 1])\n",
    "        ax.axis('off')\n",
    "\n",
    "#=========================================================================================        \n",
    "        \n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data\n",
    "\n",
    "To visualize the data that you imported, randomly selects 100 rows from `X` and passes those rows to the `displayData` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 100 data points to display\n",
    "rand_indices = np.random.choice(m, 6, replace=False)\n",
    "#rand_indices = np.arange(990,1010)\n",
    "sel = X[rand_indices, :]\n",
    "print(y[rand_indices])\n",
    "\n",
    "displayData(sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading an image and converting it into 1D array of size (1 X 400)\n",
    "\n",
    "To load an arbitrary image, resizing it to 20 X 20 pixel image, converting it into a grayscale image and lastly storing as a an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_image1 = Image.open(\"sample.jpg\") # Loading image\n",
    "an_image = an_image1.resize((20,20)) # Resizing\n",
    "\n",
    "grayscale_image = an_image.convert(\"L\") # Conversion into grayscale\n",
    "grayscale_array = np.asarray(grayscale_image) # Conversion into an array\n",
    "\n",
    "grayscale_1D = np.ravel(grayscale_array) # Conversion into 1D array\n",
    "\n",
    "#pyplot.imshow(grayscale_array, cmap=\"gray\")\n",
    "displayData(grayscale_1D,figsize=(3,3))\n",
    "print(grayscale_1D.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "#### Vectorizing the cost function \n",
    "\n",
    "We will begin by writing a vectorized version of the cost function. Recall that in (unregularized) logistic regression, the cost function is\n",
    "\n",
    "$$ J(w) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)} \\log \\left( h_w\\left( x^{(i)} \\right) \\right) - \\left(1 - y^{(i)} \\right) \\log \\left(1 - h_w \\left( x^{(i)} \\right) \\right) \\right] $$\n",
    "\n",
    "To compute each element in the summation, we have to compute $h_w(x^{(i)})$ for every example $i$, where $h_w(x^{(i)}) = g(w^T x^{(i)})$ and $g(z) = \\frac{1}{1+e^{-z}}$ is the sigmoid function. It turns out that we can compute this quickly for all our examples by using matrix multiplication. Let us define $X$ and $w$ as\n",
    "\n",
    "$$ X = \\begin{bmatrix} - \\left( x^{(1)} \\right)^T - \\\\ - \\left( x^{(2)} \\right)^T - \\\\ \\vdots \\\\ - \\left( x^{(m)} \\right)^T - \\end{bmatrix} \\qquad \\text{and} \\qquad w = \\begin{bmatrix} w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_n \\end{bmatrix} $$\n",
    "\n",
    "Then, by computing the matrix product $Xw$, we have: \n",
    "\n",
    "$$ Xw = \\begin{bmatrix} - \\left( x^{(1)} \\right)^Tw - \\\\ - \\left( x^{(2)} \\right)^Tw - \\\\ \\vdots \\\\ - \\left( x^{(m)} \\right)^Tw - \\end{bmatrix} = \\begin{bmatrix} - w^T x^{(1)}  - \\\\ - w^T x^{(2)} - \\\\ \\vdots \\\\ - w^T x^{(m)}  - \\end{bmatrix} $$\n",
    "\n",
    "In the last equality, we used the fact that $a^Tb = b^Ta$ if $a$ and $b$ are vectors. This allows us to compute the products $w^T x^{(i)}$ for all our examples $i$ in one line of code.\n",
    "\n",
    "#### Vectorizing the gradient\n",
    "\n",
    "Recall that the gradient of the (unregularized) logistic regression cost is a vector where the $j^{th}$ element is defined as\n",
    "\n",
    "$$ \\frac{\\partial J }{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( \\left( h_w\\left(x^{(i)}\\right) - y^{(i)} \\right)x_j^{(i)} \\right) $$\n",
    "\n",
    "To vectorize this operation over the dataset, we start by writing out all the partial derivatives explicitly for all $w_j$,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial J}{\\partial w_0} \\\\\n",
    "\\frac{\\partial J}{\\partial w_1} \\\\\n",
    "\\frac{\\partial J}{\\partial w_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J}{\\partial w_n}\n",
    "\\end{bmatrix} = &\n",
    "\\frac{1}{m} \\begin{bmatrix}\n",
    "\\sum_{i=1}^m \\left( \\left(h_w\\left(x^{(i)}\\right) - y^{(i)} \\right)x_0^{(i)}\\right) \\\\\n",
    "\\sum_{i=1}^m \\left( \\left(h_w\\left(x^{(i)}\\right) - y^{(i)} \\right)x_1^{(i)}\\right) \\\\\n",
    "\\sum_{i=1}^m \\left( \\left(h_w\\left(x^{(i)}\\right) - y^{(i)} \\right)x_2^{(i)}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\sum_{i=1}^m \\left( \\left(h_w\\left(x^{(i)}\\right) - y^{(i)} \\right)x_n^{(i)}\\right) \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "= & \\frac{1}{m} \\sum_{i=1}^m \\left( \\left(h_w\\left(x^{(i)}\\right) - y^{(i)} \\right)x^{(i)}\\right) \\\\\n",
    "= & \\frac{1}{m} X^T \\left( h_w(x) - y\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$  h_w(x) - y = \n",
    "\\begin{bmatrix}\n",
    "h_w\\left(x^{(1)}\\right) - y^{(1)} \\\\\n",
    "h_w\\left(x^{(2)}\\right) - y^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "h_w\\left(x^{(m)}\\right) - y^{(m)} \n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Note that $x^{(i)}$ is a vector, while $h_w\\left(x^{(i)}\\right) - y^{(i)}$  is a scalar (single number).\n",
    "To understand the last step of the derivation, let $\\beta_i = (h_w\\left(x^{(m)}\\right) - y^{(m)})$ and\n",
    "observe that:\n",
    "\n",
    "$$ \\sum_i \\beta_ix^{(i)} = \\begin{bmatrix} \n",
    "| & | & & | \\\\\n",
    "x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\\\\n",
    "| & | & & | \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\beta_1 \\\\\n",
    "\\beta_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_m\n",
    "\\end{bmatrix} = x^T \\beta\n",
    "$$\n",
    "\n",
    "where the values $\\beta_i = \\left( h_w(x^{(i)} - y^{(i)} \\right)$.\n",
    "\n",
    "Now the job is to define a new function (`lrCostFunction`) which will take the data (vectors `X` and `y`) and parameter (`Lambda`) as input and return the cost as a scalar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularized logistic regression\n",
    "\n",
    "Now add regularization to the cost function. Recall that for regularized logistic\n",
    "regression, the cost function is defined as\n",
    "\n",
    "$$ J(w) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)} \\log \\left(h_w\\left(x^{(i)} \\right)\\right) - \\left( 1 - y^{(i)} \\right) \\log\\left(1 - h_w \\left(x^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n w_j^2 $$\n",
    "\n",
    "Note that $w_0$ should not be regularized as it is used as bias term.\n",
    "Correspondingly, the partial derivative of regularized logistic regression cost for $w_j$ is defined as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\frac{\\partial J(w)}{\\partial w_0} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_w\\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)}  & \\text{for } j = 0 \\\\\n",
    "& \\frac{\\partial J(w)}{\\partial w_j} = \\left( \\frac{1}{m} \\sum_{i=1}^m \\left( h_w\\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} \\right) + \\frac{\\lambda}{m} w_j & \\text{for } j  \\ge 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Once you finished your implementation, you can call the function `lrCostFunction` to test your solution using the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrCostFunction(w, X, y, lambda_):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "### Multi-class Classification\n",
    "\n",
    "In this part of the exercise, we will implement multi-class classification by training multiple regularized logistic regression classifiers, one for each of the $K$ classes in our dataset.\n",
    "\n",
    "We will code for the function `oneVsAll` below, to train one classifier for each class. In particular, the code should return all the classifier parameters in a matrix $w \\in \\mathbb{R}^{K \\times (N +1)}$, where each row of $w$ corresponds to the learned logistic regression parameters for one class. One can do this with a “for”-loop from $0$ to $K-1$, training each classifier independently.\n",
    "\n",
    "The obvious approach is to use a one-versus-the-rest approach (also called one-vs-all), in which we train C binary classifiers, fc(x), where the data from class c is treated as positive, and the data from all the other classes is treated as negative.\n",
    "\n",
    "<a id=\"oneVsAll\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "print(y[490:510,])\n",
    "c1 = (y == c)\n",
    "print(c1[490:510,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneVsAll(X, y, num_labels, lambda_):\n",
    "        \n",
    "    m, n = X.shape\n",
    "    \n",
    "    all_w = np.zeros((num_labels, n + 1))\n",
    "\n",
    "    X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return all_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After complting the code for `oneVsAll`, the following cell shall use the code to train a multi-class classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.1\n",
    "all_w = oneVsAll(X, y, num_labels, lambda_)\n",
    "print(all_w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "#### Multi-class Prediction\n",
    "\n",
    "After training one-vs-all classifier, one can now use it to predict the digit contained in a given image. For each input, one should compute the “probability” that it belongs to each class using the trained logistic regression classifiers. The one-vs-all prediction function will pick the class for which the corresponding logistic regression classifier outputs the highest probability and return the class label (0, 1, ..., K-1) as the prediction for the input example.\n",
    "<a id=\"predictOneVsAll\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictOneVsAll(all_w, X):\n",
    "        \n",
    "    m = X.shape[0];\n",
    "    num_labels = all_w.shape[0]\n",
    "\n",
    "    # Return the following variable \n",
    "    p = np.zeros(m)\n",
    "\n",
    "    X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "\n",
    "    p = \n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding the above function\n",
    "\n",
    "In the example below, the size of variable `ff` will be 5000 $\\times$ 10  $\\textit{i.e.}$  using sigmoid function, the expression will compute the probability for all the classes (10 in total) for a particular example (5000 in total) by using the optimized parameters (`w_all`) . Then using the `argmax` function, the index corresponding to the maximum probability (among 10 probabilities for a particular example) will be picked up. The index will then represent the classification of the given example in one of the 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, call `predictOneVsAll` function using the learned value of $w$. One should see the training set accuracy in percentage which shows that the algorithm classifies `p%` of the examples in the training set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predictOneVsAll(all_w, X)\n",
    "print('Training Set Accuracy: {:.2f}%'.format(np.mean(pred == y) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rand_indices = np.arange(3331,3346)\n",
    "rand_indices = np.random.choice(m, 20, replace=False)\n",
    "#print(rand_indices)\n",
    "print(pred[rand_indices])\n",
    "sel = X[rand_indices, :]\n",
    "\n",
    "displayData(sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
